// lib/blog-content/strategies-eviter-blocage-scraping.ts
import { ArticleSectionType } from '@/components/blog/ArticleContent'

export const strategiesBlocageContent: ArticleSectionType[] = [
    {
        type: "intro",
        content: "Le scraping web est devenu un outil stratÃ©gique incontournable pour les marketeurs, les e-commerÃ§ants et les analystes. Mais si vous avez dÃ©jÃ  tentÃ© d'extraire des donnÃ©es d'un site, vous avez sÃ»rement rencontrÃ© le fameux message : âŒ 'Access Denied' ou 'Too Many Requests (429)'."
      },
      {
        type: "paragraph",
        content: "Les blocages de scraping sont frÃ©quents, mais pas inÃ©vitables. Dans cet article, dÃ©couvrons les meilleures stratÃ©gies pour scraper en toute sÃ©curitÃ© et sans interruption, mÃªme sur les plateformes les plus protÃ©gÃ©es."
      },
      {
        type: "section",
        emoji: "ğŸ¤”",
        title: "Pourquoi les sites bloquent le scraping",
        content: "Les sites web mettent en place des mÃ©canismes anti-scraping pour protÃ©ger leurs donnÃ©es, limiter les abus ou prÃ©server leurs serveurs. Voici les causes les plus courantes :",
        list: [
          "Trop de requÃªtes envoyÃ©es en peu de temps",
          "IP dÃ©tectÃ©e comme suspecte", 
          "Absence d'en-tÃªtes 'humains' (User-Agent, Referrer, etc.)",
          "Navigation non simulÃ©e (pas de cookies, pas de dÃ©lais)",
          "Contournement des systÃ¨mes de connexion ou de captcha"
        ]
      },
      {
        type: "tip",
        content: "ğŸ’¡ En clair : si votre robot agit trop vite ou trop 'parfaitement', il est vite repÃ©rÃ©."
      },
      {
        type: "section",
        emoji: "ğŸ§©",
        title: "1. RÃ©guler la frÃ©quence des requÃªtes",
        content: "Le premier rÃ©flexe est d'imiter le comportement humain. Au lieu de lancer 1000 requÃªtes Ã  la seconde, ralentissez le rythme :",
        list: [
          "Ajoutez un dÃ©lai alÃ©atoire (par exemple 2 Ã  6 secondes entre chaque requÃªte)",
          "Variez l'ordre des pages visitÃ©es", 
          "Ã‰vitez de scraper le mÃªme site en continu pendant des heures"
        ],
        example: "ğŸ’¡ Astuce Scrapdeouf : nos robots intÃ¨grent un systÃ¨me de 'random delay' automatique pour simuler une navigation naturelle."
      },
      {
        type: "section", 
        emoji: "ğŸ•µï¸â€â™‚ï¸",
        title: "2. Utiliser des en-tÃªtes (headers) rÃ©alistes",
        content: "Les serveurs identifient souvent les bots Ã  cause de leurs en-tÃªtes HTTP trop 'propres' ou gÃ©nÃ©riques. Ajoutez des champs pour ressembler Ã  un vrai navigateur :",
        code: `headers: {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
    "Accept-Language": "fr-FR,fr;q=0.9", 
    "Referer": "https://www.google.com"
  }`,
        example: "ğŸ’¡ Scrapdeouf simule automatiquement des user-agents rÃ©alistes pour chaque session."
      },
      {
        type: "section",
        emoji: "ğŸŒ", 
        title: "3. Faire tourner les adresses IP (proxy rotation)",
        content: "L'un des moyens les plus efficaces pour Ã©viter les blocages est d'utiliser des proxies rotatifs. Chaque requÃªte passe par une IP diffÃ©rente, ce qui empÃªche le site de repÃ©rer un comportement anormal.",
        list: [
          "ğŸŒ Proxy rÃ©sidentiel â†’ idÃ©al pour passer inaperÃ§u",
          "ğŸ¢ Proxy datacenter â†’ rapide mais plus dÃ©tectable", 
          "âš™ï¸ Proxy rotatif â†’ change d'IP automatiquement Ã  chaque requÃªte"
        ],
        example: "ğŸ’¡ Avec Scrapdeouf, vous n'avez pas Ã  gÃ©rer de proxies manuellement : notre infrastructure s'en charge."
      },
      {
        type: "section",
        emoji: "ğŸ”„",
        title: "4. Simuler la navigation humaine", 
        content: "Un humain ne clique pas toujours au mÃªme rythme, ni dans le mÃªme ordre. Un bon scraper doit reproduire ce comportement :",
        list: [
          "Charger les pages comme un vrai navigateur",
          "Scroller, cliquer, attendre un temps alÃ©atoire",
          "GÃ©rer les cookies et sessions"
        ],
        example: "ğŸ’¡ Plus votre scraping est 'humain', moins vous serez bloquÃ©."
      },
      {
        type: "section",
        emoji: "ğŸ§±",
        title: "5. GÃ©rer les CAPTCHA intelligemment",
        content: "Les CAPTCHAs (du type 'Je ne suis pas un robot') sont le cauchemar du scrapper. Pour les contourner proprement, il existe plusieurs approches :", 
        list: [
          "Anticiper : scraper avant qu'un captcha n'apparaisse",
          "Ã‰viter : utiliser un navigateur headless lÃ©gitime", 
          "RÃ©soudre : via une API tierce ou un service intÃ©grÃ©"
        ],
        example: "ğŸ’¡ Scrapdeouf utilise une approche d'Ã©vitement automatisÃ©e pour contourner la majoritÃ© des CAPTCHAs sans intervention humaine."
      },
      {
        type: "section",
        emoji: "ğŸ§ ",
        title: "6. Varier les patterns d'accÃ¨s",
        content: "Un robot prÃ©visible, c'est un robot dÃ©tectÃ©. Modifiez rÃ©guliÃ¨rement vos schÃ©mas de scraping :",
        list: [
          "Changez les heures de scraping",
          "Alternez entre plusieurs sources de donnÃ©es",
          "Ã‰vitez d'accÃ©der toujours aux mÃªmes URLs dans le mÃªme ordre"
        ],
        example: "ğŸ’¡ Scrapdeouf varie automatiquement les patterns d'accÃ¨s pour chaque session utilisateur."
      },
      {
        type: "section",
        emoji: "âš™ï¸", 
        title: "7. Utiliser le bon timing",
        content: "Les sites ont souvent des pÃ©riodes de charge (journÃ©e, soirÃ©es, week-ends). Scraper pendant les heures creuses (tÃ´t le matin ou la nuit) rÃ©duit considÃ©rablement les risques de dÃ©tection.",
        example: "ğŸ’¡ Planifiez vos extractions sur Scrapdeouf pendant les plages Ã  faible trafic pour des performances maximales."
      },
      {
        type: "section",
        emoji: "ğŸ§©",
        title: "8. Surveiller les rÃ©ponses serveur", 
        content: "Surveillez toujours les codes de statut HTTP renvoyÃ©s par les sites :",
        list: [
          "200 = OK",
          "403 = accÃ¨s refusÃ© (vous Ãªtes bloquÃ©)", 
          "429 = trop de requÃªtes",
          "503 = service indisponible"
        ],
        example: "ğŸ’¡ Scrapdeouf gÃ¨re ces statuts automatiquement et adapte la frÃ©quence d'extraction en temps rÃ©el."
      },
      {
        type: "section",
        emoji: "ğŸ§°",
        title: "9. Ne pas tout scraper d'un coup",
        content: "Inutile de vouloir tout extraire en une seule session. Divisez vos scrapes en petites campagnes programmÃ©es :",
        list: [
          "Moins de charge sur le serveur distant",
          "Moins de risque de blocage", 
          "RÃ©sultats plus stables et fiables"
        ],
        example: "ğŸ’¡ Scrapdeouf permet de planifier vos collectes par lots sans surcharge."
      },
      {
        type: "section",
        emoji: "ğŸ”’",
        title: "10. Utiliser une plateforme spÃ©cialisÃ©e (comme Scrapdeouf.com)",
        content: "CrÃ©er votre propre scraper, c'est formateurâ€¦ mais risquÃ© et chronophage. Une plateforme SaaS comme Scrapdeouf.com intÃ¨gre dÃ©jÃ  :",
        list: [
          "âœ… Gestion automatique des IPs et des dÃ©lais",
          "âœ… Simulation de navigation rÃ©elle", 
          "âœ… Anti-blocage intelligent",
          "âœ… Suivi des sessions et reprise aprÃ¨s erreur"
        ],
        example: "RÃ©sultat : vous vous concentrez sur les donnÃ©es, pas sur les blocages."
      },
      {
        type: "table",
        title: "ğŸ“Š En rÃ©sumÃ©",
        headers: ["ProblÃ¨me courant", "Solution recommandÃ©e"],
        rows: [
          ["Trop de requÃªtes", "Ajouter des dÃ©lais alÃ©atoires"],
          ["IP bloquÃ©e", "Utiliser des proxies rotatifs"],
          ["CAPTCHA rÃ©current", "Simuler navigation humaine"], 
          ["DÃ©tection User-Agent", "Ajouter des headers rÃ©alistes"],
          ["RequÃªtes 429/403", "Adapter rythme + changer IP"]
        ]
      },
      {
        type: "conclusion",
        title: "ğŸ’¬ Conclusion",
        content: [
          "Le scraping n'est pas une question de vitesse, mais d'intelligence. En appliquant ces stratÃ©gies, vous pouvez scraper efficacement sans jamais Ãªtre bloquÃ©, mÃªme sur les plateformes les plus complexes.",
          "Et si vous prÃ©fÃ©rez aller plus vite, Scrapdeouf.com s'occupe de tout : ğŸš€ Anti-blocage intÃ©grÃ©, ğŸŒ Proxies rotatifs, ğŸ§  Scraping intelligent, ğŸ“ˆ RÃ©sultats fiables Ã  100 %.",
          "ğŸ‘‰ Essayez Scrapdeouf dÃ¨s aujourd'hui et dÃ©couvrez Ã  quel point la collecte de donnÃ©es peut Ãªtre simple, rapide et sans limites."
        ]
      }
]